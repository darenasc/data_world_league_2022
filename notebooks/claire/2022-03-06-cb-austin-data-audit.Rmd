---
title: "R Notebook"
output: html_notebook
---
# Overview

# Set up 

```{r libraries}
library(tidyverse)
```

```{r read data}
pop <- read_csv(here::here('data', 'austin_pop.csv'))

waste <- read.csv(here::here('data', 'Waste_Collection___Diversion_Report__daily_.csv'), sep = ';') %>%
  mutate(report_date = lubridate::mdy(ï..Report.Date),
         Load.Time = lubridate::mdy_hms(Load.Time),
         Load.Weight = as.numeric(str_remove_all(Load.Weight, ','))) %>%
  select(-ï..Report.Date)
```

# Dataset exploration

Here are are a few observation on the data:

* `Load.Time` is described as a weight in pounds in the data dictionary, that's probably a mistake as it's a `datetime`
* `Load.Weight` is described as a weight in tons but 11764 tons per load sounds like a lot? It's most likely to be in pounds.

```{r summary}
waste %>% summary()
```

## Let's look for the primary key in the ds

The data dictionary says that every row is a load but there are more rows than load IDs.
```{r}
length(unique(waste$Load.ID))
```

```{r}
unique_waste <- waste %>% unique()
```

There were 3 duplicated rows, there are more rows with duplicated IDs.

```{r}
dup_id <- unique_waste %>% count(Load.ID) %>% filter(n > 1) %>% arrange(-n)
```

It seems that most rows with duplicated `Load.ID` are the same for every other column except for `Route.Number` so it is fair to assume that they are duplicates that could be removed altogether or randomly assigned to one of the routes.

```{r}
unique_waste %>% 
  filter(Load.ID %in% dup_id$Load.ID) %>%
  group_by(Load.ID) %>%
  summarise(n_rows = n(),
            n_routes = n_distinct(Route.Number),
            n_load_weight = n_distinct(Load.Weight)) %>%
  arrange(n_routes) %>%
  View()

```

It seems that for these 2 the report date is different but I think it's the same instance.

```{r}
View(waste %>% filter(Load.ID %in% c(103691, 726111)))
```

Recommendation for duplicated rows: it's only 41 rows out of 740k - I'd randomly pick the first row of each duplicated group of rows.

```{r create a uniquely identify dataset}
unique_waste <- unique_waste %>%
  group_by(Load.ID) %>%
  summarise_all(first) %>%
  ungroup()
```

## Missing values

72335 observations are missing in the `Load.Weight`

Let's see how they are distributed
```{r}
unique_waste %>% 
  group_by(Load.Type) %>% 
  summarise(prop_missing = sum(is.na(Load.Weight)) / n()) %>% 
  arrange(-prop_missing)
```

* 81% of the `SWEEPING` observations are missing, I suggest we remove it from the analysis.
* 5% of `YARD TRIMMING - X-MAS TREES` is missing, it's only 17 rows so I'd also remove it from the analysis.
* for other categories, I'd simply remove rows with missing load weight.


```{r dealing with missing values}
no_missing_waste <- unique_waste %>% filter(!is.na(Load.Weight),
                                      !Load.Type %in% c('SWEEPING', 'YARD TRIMMING - X-MAS TREES'))
```

## Other data errors and checks

Let's plot the data to have a better idea of the issues we need to address in the cleaning.

```{r grouping per week}
create_daily_grouping <- function(df){
  df %>%
    mutate(day = lubridate::floor_date(lubridate::as_date(Load.Time), unit = 'day')) %>%
    group_by(day, Load.Type) %>%
    summarise(med_weight = median(Load.Weight, na.rm = T),
              av_weight = mean(Load.Weight, na.rm =T),
              load_nb = n_distinct(Load.ID),
              trucks_nb = n_distinct(Route.Number)) %>%
    ungroup()
    
}

day_grouped_waste <- no_missing_waste %>% create_daily_grouping()
  
```


```{r}
plot_ts_by_type <- function(grouped_df, start_year=2000, end_year=2021, categories_list = no_missing_waste$Load.Type){
  grouped_df %>% 
  filter(lubridate::year(day) >= start_year,
         lubridate::year(day) <= end_year,
         Load.Type %in% categories_list) %>%
  ggplot(aes(x = day, y = med_weight)) +
  geom_point() +
  theme(axis.text.x = element_text(angle=45)) +
  facet_wrap(Load.Type ~ .) +
  labs(title = 'Median waste weight per week',
       subtitle = 'broken down by load type',
       x = NULL,
       y = NULL)
}

plot_ts_by_type(day_grouped_waste)

```

Here are some issues that need to be addressed with the data:

1) `Load.Type` categories don't seem mutually exclusive.

This could be because over time, recycling streams got split into different categories or some categories changed in the way the data is collected. We can manually pick the load types we want to include in the analysis. In this case, I recommend keeping: *Brush, Bulk, Dead animal, Garbage collections, Litter, Mixed Litter, Recycled Metal, Recycling - single stream, Tires and Yard Trimming.*

2) The time series is incomplete for some load types. We need to decide on a cut off date. I recommend *2005*

3) There are some extreme values in the load weight that need to be removed from the analysis. I recommend using an outlier detection technique de decide on what observations are likely to be data errors, or outliers not containing information about the time series.

We will apply the outlier detection below. Here is the plot of the data, taking into account the recommendations above.

```{r}
load_type_list <- c('RECYCLING - SINGLE STREAM', 'BULK', 'YARD TRIMMING', 'BRUSH', 'GARBAGE COLLECTIONS', 'TIRES', 'LITTER', 'MIXED LITTER', 'DEAD ANIMAL', 'RECYCLED METAL')
  
plot_ts_by_type(day_grouped_waste, start_year = 2005, end_year = 2021, categories_list = load_type_list)
```
Outliers are flattening the plot. We need to remove them from the analysis.

```{r applying rules to keep only relevant Types and Dates}
selected_waste <- no_missing_waste %>%
  filter(Load.Type %in% load_type_list,
         lubridate::year(Load.Time) >= 2005,
         lubridate::year(Load.Time) <= 2021)
```

```{r observing incoherent data points}
selected_waste %>% 
  count(Load.Type, Route.Type) %>% 
  mutate(coherent = str_detect(Load.Type, Route.Type)) %>% # the route type includes the load type 
  # group_by(coherent) %>%
  # summarise(total = sum(n)) %>%
  View()
```

One last issue is that some route types are not matching the load type they are collecting. 

It seems to be quite frequent, although not very common. Could it be that indeed a route that's supposed to collect metal collected some yard trimmings? Or is it a manual data error? This could be dealt with by an outlier detection algorithm that would get rid of observations that are likely to be data errors.

## Outliers removal

There is one load weight value that is negative in a recycling collection.

```{r}
selected_waste %>% 
  filter(Load.Type == 'GARBAGE COLLECTIONS', # checking the most common type
         Load.Weight > 0, 
         Load.Weight < 50000) %>% # 25 tons
  ggplot(aes(Load.Weight)) +
  geom_histogram()
```

There are a few values that are close to 0 or exactly 0 - they might not be a problem

```{r}
selected_waste %>% 
  filter(Load.Weight <= 1)  %>%
  arrange(-Load.Weight)
```

Rather than making arbitrary rules to identify outliers, we can use an outlier detection method to make rules that are statistically sensible.

```{r}
detect_outliers <- function(load_type){

  df <- selected_waste %>%
    filter(Load.Type == load_type) %>% 
    select(Load.ID, Load.Weight)

  
  iforest <- solitude::isolationForest$new()

  iforest$fit(df)
  
  #predict outliers within dataset
  df$pred <- iforest$predict(df)
  
  threshold <- df$pred$anomaly_score %>% quantile(0.97)
  
  df$outlier <- as.factor(ifelse(df$pred$anomaly_score >= threshold, "outlier", "normal"))
  
  df %>%
    select(Load.ID, Load.Weight, outlier)
}
```

```{r}
outliers_predictions <- purrr::map_dfr(load_type_list, detect_outliers)
```
```{r}
selected_waste_w_outliers <- selected_waste %>%
  left_join(outliers_predictions, by = c('Load.ID', 'Load.Weight')) %>%
  filter(Load.Weight < 50000) # removing the one point that's off the chart

selected_waste_w_outliers %>%
  ggplot(aes(Load.Type, Load.Weight, colour = outlier)) + 
  ggplot2::geom_jitter(aes(colour = outlier), size=0.4, alpha=0.7) +
  coord_flip() +
  labs(title = 'Outliers per load types',
       x = 'Load weight in pounds',
       y = NULL)
```

```{r}
grouped_waste_no_outlier <- selected_waste %>%
  left_join(outliers_predictions, by = c('Load.ID', 'Load.Weight')) %>%
  filter(outlier != 'outlier') %>%
  create_daily_grouping() 

grouped_waste_no_outlier %>%
  filter(lubridate::year(week) > 2004,
         week <= '2021-06-01') %>%
  ggplot(aes(week, av_weight)) +
  geom_line() +
  theme(axis.text.x = element_text(angle = 45)) +
  facet_wrap(Load.Type ~.)+
  labs(title = 'Time series of different types of waste',
       subtitle = 'Outliers and missing data cleaned',
       x = NULL,
       y = 'Average weekly waste in pounds')
```

2 dates are in the future (removed) and sometimes the report date is years after the load.time - that seems strange? Maybe we just ignore `report_date`

```{r}
time_discrep <- selected_waste %>%
  mutate(time_diff = lubridate::interval(lubridate::as_date(Load.Time), report_date)/ lubridate::days(1)) %>%
  filter(time_diff != 0) 

time_discrep %>% 
  ggplot(aes(time_diff)) +
  geom_density()
```

# Create features

```{r}
clean_df <- selected_waste_w_outliers %>% 
  mutate(date = lubridate::as_date(lubridate::floor_date(Load.Time, unit = 'day')),
         year = lubridate::year(Load.Time),
         month = lubridate::month(Load.Time),
         wday = lubridate::wday(Load.Time, abbr = T, label = T)) %>%
  left_join(pop %>% select(year, total_pop, annualised_growth), by = 'year') %>%
  group_by(date, year, month, wday, Load.Type, Route.Type, Dropoff.Site, Route.Number, outlier, total_pop, annualised_growth) %>%
  summarise(nb_loads = n_distinct(Load.ID),
            daily_weight = sum(Load.Weight)) %>%
  ungroup()
clean_df
```
```{r}
write_csv(clean_df, here::here('data', 'clean_waste_data.csv'))
```


